{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "4371e767"
   },
   "source": [
    "# 05c: CNN-LSTM Model Optimization and Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation and interpretability analysis of the trained CNN-LSTM model for ICU mortality prediction, focusing on model performance assessment and clinical insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "ugbDkCaafV64"
   },
   "source": [
    "#### 1. Imports and Configuration\n",
    "\n",
    "Import all necessary libraries for model evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "23fa2d18"
   },
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "FvsOrPAifTlo"
   },
   "source": [
    "## 2. Load Trained CNN-LSTM Model\n",
    "This section loads the trained CNN-LSTM model from disk. The model was previously trained on ICU patient data and saved for downstream evaluation and interpretability analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a37fe73f",
    "outputId": "d49a7d25-9693-4dea-e1d3-4be2dd4ab3af"
   },
   "outputs": [],
   "source": [
    "# Define the path to the saved model\n",
    "model_path_to_load = '/content/sample_data/enhanced_model_36hour.keras'  # Or 'base_model_36hour.keras'\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(model_path_to_load)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "PAZ2sYmofva9"
   },
   "source": [
    "## 3. Load Test Data for Evaluation\n",
    "This section loads the preprocessed test data required for evaluating the trained CNN-LSTM model. The test data includes time series features, static patient features, and ground truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "FCUH_qgbb0Bq"
   },
   "outputs": [],
   "source": [
    "class KerasModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X, verbose=0).flatten() > 0.5).astype(int)\n",
    "    def predict_proba(self, X):\n",
    "        proba = self.model.predict(X, verbose=0).flatten()\n",
    "        return np.vstack([1 - proba, proba]).T  # shape: (n_samples, 2)\n",
    "\n",
    "keras_model = KerasModelWrapper(loaded_model)\n",
    "\n",
    "data = np.load('../data/processed/cnn_lstm_36hour_data.npz', allow_pickle=True)\n",
    "X_test_final = data['X_test']\n",
    "static_test_final = data['static_test']\n",
    "y_test_final = data['y_test']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "1buAV42wfxhX"
   },
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "308Xe6elgBs6"
   },
   "source": [
    "**Confusion Matrix Results:**\n",
    "\n",
    "- **True Negatives (Top-Left):** 390 cases were correctly predicted as negative (survived).\n",
    "- **False Positives (Top-Right):** 299 cases were incorrectly predicted as positive (died) when they actually survived. This represents Type I errors.\n",
    "- **False Negatives (Bottom-Left):** 19 cases were incorrectly predicted as negative (survived) when they actually died. This represents Type II errors.\n",
    "- **True Positives (Bottom-Right):** 92 cases were correctly predicted as positive (died).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "vrYNI7vFcKjA",
    "outputId": "229b0fae-a578-41f2-b75b-5668f195c688"
   },
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "print(\"Generating Confusion Matrix...\")\n",
    "\n",
    "# Use the wrapped model to get predictions\n",
    "y_pred = keras_model.predict([X_test_final, static_test_final])\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_final, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('CNN-LSTM Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "IHdW1_wBgWmp"
   },
   "source": [
    "**Precision-Recall Curve:**\n",
    "\n",
    "The Precision-Recall curve shows the trade-off between precision and recall at different probability thresholds.\n",
    "\n",
    "*   **Precision** is the ability of the classifier not to label as positive a sample that is negative.\n",
    "*   **Recall** is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The **Area Under the Precision-Recall Curve (AUPRC)** is a single number summarizing the curve. A higher AUPRC indicates better performance, especially in imbalanced datasets where the positive class is rare. In this case, the AUPRC of 0.36 suggests that the model's ability to achieve high precision and high recall simultaneously is limited. This aligns with the confusion matrix results which showed a relatively high number of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "8Pl6FoBWc2lc",
    "outputId": "b1b72f5e-dffb-48df-b820-d3389ab46d69"
   },
   "outputs": [],
   "source": [
    "print(\"Generating Precision-Recall Curve...\")\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_proba = keras_model.predict_proba([X_test_final, static_test_final])[:, 1]\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_test_final, y_proba)\n",
    "\n",
    "# Calculate Area Under the Precision-Recall Curve (AUPRC)\n",
    "auprc = auc(recall, precision)\n",
    "\n",
    "# Plot the Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall Curve (AUPRC = {auprc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('CNN-LSTM - Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precision-Recall Curve generated with AUPRC: {auprc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "liudCXAegrZD"
   },
   "source": [
    "**ROC Curve:**\n",
    "\n",
    "The ROC curve shows the model's ability to discriminate between positive and negative classes across various thresholds. The **Area Under the ROC Curve (AUROC)** quantifies this discriminatory power, with a value closer to 1 indicating better performance. In this case, an AUROC of 0.77 suggests that the model has moderate discriminatory ability. The curve being above the random guess line indicates that the model performs better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "feoeALsac7TM",
    "outputId": "8ce453f8-88df-4481-cdbe-257bc5898813"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generating ROC Curve...\")\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_proba = keras_model.predict_proba([X_test_final, static_test_final])[:, 1]\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y_test_final, y_proba)\n",
    "\n",
    "# Calculate Area Under the ROC Curve (AUROC)\n",
    "auroc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUROC = {auroc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess') # Add random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('CNN-LSTM - ROC Curve  ')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC Curve generated with AUROC: {auroc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "sCLfS1qddc3B"
   },
   "outputs": [],
   "source": [
    "# Dynamic (time series) features\n",
    "time_series_features = [\n",
    "    'HR', 'SysABP', 'DiasABP', 'MAP', 'NISysABP', 'NIDiasABP', 'NIMAP', 'MechVent',\n",
    "    'RespRate', 'SaO2', 'FiO2', 'PaO2', 'PaCO2',\n",
    "    'Creatinine', 'BUN', 'Urine',\n",
    "    'Na', 'K', 'Glucose', 'Lactate', 'HCO3', 'pH',\n",
    "    'GCS', 'Temp',\n",
    "    # Clinically meaningful combinations of vitals\n",
    "    'Shock Index',          # HR / SysABP\n",
    "    'PaO2/FiO2 ratio',      # PaO₂ / FiO₂\n",
    "    'Pulse Pressure',       # SysABP − DiasABP\n",
    "    'MAP to HR ratio'       # MAP / HR\n",
    "]\n",
    "\n",
    "# Static features\n",
    "static_features = [\n",
    "    'Age', 'Gender', 'Height', 'Weight', 'ICUType', 'SAPS-I', 'SOFA'\n",
    "]\n",
    "\n",
    "target_col = 'In-hospital_death'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vsCHCkpdgsp",
    "outputId": "3f49a08f-961a-4e8c-c358-caef4c67580e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load trained model and test data\n",
    "model_path = '../data/processed/enhanced_model_36hour.keras'\n",
    "data_path = '../data/processed/cnn_lstm_36hour_data.npz'\n",
    "\n",
    "try:\n",
    "    # Load trained CNN-LSTM model\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    # Load preprocessed test data\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    X_test_final = data['X_test']\n",
    "    static_test_final = data['static_test']\n",
    "    y_test_final = data['y_test']\n",
    "\n",
    "    print(f\"Data loaded from {data_path}\")\n",
    "    print(f\"  Time series shape: {X_test_final.shape}\")\n",
    "    print(f\"  Static features shape: {static_test_final.shape}\")\n",
    "    print(f\"  Labels shape: {y_test_final.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Prepare data for SHAP analysis\n",
    "background_size = 50\n",
    "sample_size = 50\n",
    "\n",
    "# Load all data splits for comprehensive analysis\n",
    "X_train_final = data['X_train']\n",
    "X_val_final = data['X_val']\n",
    "static_train_final = data['static_train']\n",
    "static_val_final = data['static_val']\n",
    "\n",
    "if X_test_final.shape[0] >= background_size + sample_size:\n",
    "    # Create background and sample sets from test data\n",
    "    X_background = X_test_final[:background_size]\n",
    "    static_background = static_test_final[:background_size]\n",
    "    X_sample = X_test_final[background_size:background_size + sample_size]\n",
    "    static_sample = static_test_final[background_size:background_size + sample_size]\n",
    "\n",
    "    print(f\"\\n SHAP datasets prepared:\")\n",
    "    print(f\"  Background samples: {background_size}\")\n",
    "    print(f\"  Analysis samples: {sample_size}\")\n",
    "\n",
    "    # Average time series over temporal dimension for SHAP\n",
    "    X_background_mean = np.mean(X_background, axis=1)\n",
    "    X_sample_mean = np.mean(X_sample, axis=1)\n",
    "\n",
    "    # Concatenate time series and static features\n",
    "    background_concat = np.concatenate([X_background_mean, static_background], axis=1)\n",
    "    sample_concat = np.concatenate([X_sample_mean, static_sample], axis=1)\n",
    "\n",
    "    print(f\"  Background concatenated shape: {background_concat.shape}\")\n",
    "    print(f\"  Sample concatenated shape: {sample_concat.shape}\")\n",
    "\n",
    "    # Create feature names for interpretability\n",
    "    if 'time_series_features' in locals() and 'static_features' in locals():\n",
    "        feature_names = time_series_features + static_features\n",
    "        if len(feature_names) != sample_concat.shape[1]:\n",
    "            print(f\"Feature mismatch: {len(feature_names)} names vs {sample_concat.shape[1]} features\")\n",
    "            feature_names = [f'feature_{i}' for i in range(sample_concat.shape[1])]\n",
    "    else:\n",
    "        feature_names = [f'feature_{i}' for i in range(sample_concat.shape[1])]\n",
    "        print(\"  Using generic feature names\")\n",
    "\n",
    "    print(f\"Feature names created: {len(feature_names)} features\")\n",
    "\n",
    "else:\n",
    "    print(f\"Insufficient test samples: {X_test_final.shape[0]} available, {background_size + sample_size} needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "a317ac85",
    "outputId": "628fc7dd-e985-4792-96b3-e8498ccc80c2"
   },
   "outputs": [],
   "source": [
    "# SHAP Force Plot - Individual Patient Analysis\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'shap_values' in locals() and 'sample_concat' in locals():\n",
    "    print(\"Generating SHAP force plot...\")\n",
    "\n",
    "    # Select patient instance to analyze\n",
    "    instance_index = 0\n",
    "    if instance_index < sample_concat.shape[0]:\n",
    "        print(f\"Analyzing patient {instance_index}...\")\n",
    "\n",
    "        # Handle 3D SHAP values\n",
    "        if len(np.shape(shap_values)) == 3:\n",
    "            if np.shape(shap_values)[2] > 1:\n",
    "                shap_values_2d = shap_values[:, :, 1]  # Positive class\n",
    "                print(f\"Using positive class SHAP values. Shape: {np.shape(shap_values_2d)}\")\n",
    "            else:\n",
    "                shap_values_2d = shap_values[:, :, 0]  # Single output\n",
    "                print(f\"Using single output SHAP values. Shape: {np.shape(shap_values_2d)}\")\n",
    "        else:\n",
    "            shap_values_2d = shap_values\n",
    "            print(f\"Using 2D SHAP values. Shape: {np.shape(shap_values_2d)}\")\n",
    "\n",
    "        # Extract values for selected patient\n",
    "        shap_values_instance = shap_values_2d[instance_index, :]\n",
    "        feature_values_instance = sample_concat[instance_index, :]\n",
    "\n",
    "        # Ensure feature names are available\n",
    "        if 'feature_names' not in locals():\n",
    "            print(\"Warning: Using generic feature names.\")\n",
    "            feature_names = [f'feature_{i}' for i in range(sample_concat.shape[1])]\n",
    "\n",
    "        # Get base value from explainer\n",
    "        if 'explainer' in locals():\n",
    "            base_value = explainer.expected_value\n",
    "            print(f\"Base value (average prediction): {base_value:.4f}\")\n",
    "        else:\n",
    "            print(\"Warning: Explainer not found, using fallback base value.\")\n",
    "            if 'predict_fn' in locals() and 'background_concat' in locals():\n",
    "                base_value = np.mean(predict_fn(background_concat))\n",
    "                print(f\"Calculated base value: {base_value:.4f}\")\n",
    "            else:\n",
    "                base_value = 0.0\n",
    "                print(\"Using default base value: 0.0\")\n",
    "\n",
    "        # Generate force plot\n",
    "        shap.force_plot(base_value, shap_values_instance, feature_values_instance,\n",
    "                        feature_names=feature_names, matplotlib=True)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Instance {instance_index} out of bounds for {sample_concat.shape[0]} samples.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: SHAP values not found. Run SHAP calculation cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "569aa060227a4bc8bc6fa20e55583abb",
      "0795cde042b74197a2b9ac3c02b95852",
      "b90743d1cfed47779ac2939e3a8551fe",
      "1d686ccda71d40beae667f244c3882c3",
      "9fe24eab0b4e45d7852893a2ac406598",
      "a12bb61ab6de41f685fe71c144c6bdfd",
      "31b1e812484f43d98690ab0ea7b9e230",
      "549165d032574db08524c25a01fea4d2",
      "0c97cc04bf4f4711a95e14a496759797",
      "81e1f353bb1b4b4f8e6b4656a5c9f13c",
      "fe44961e4a304d5db6cf4458f2261598"
     ]
    },
    "id": "6f6f2aa9",
    "outputId": "21cdf1d4-7b3c-472d-d35a-955f4dc4565c"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting SHAP analysis...\")\n",
    "\n",
    "def predict_fn(data):\n",
    "    \"\"\"\n",
    "    Prediction wrapper for multi-input CNN-LSTM model.\n",
    "    Splits concatenated features back into time series and static components.\n",
    "    \"\"\"\n",
    "    n_ts_features = len(time_series_features)\n",
    "\n",
    "    # Split concatenated data\n",
    "    ts_data = data[:, :n_ts_features]\n",
    "    static_data = data[:, n_ts_features:]\n",
    "\n",
    "    # Expand time series to original temporal dimension\n",
    "    timesteps = X_test_final.shape[1]\n",
    "    ts_data_expanded = np.repeat(ts_data[:, np.newaxis, :], timesteps, axis=1)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict([ts_data_expanded, static_data], verbose=0)\n",
    "    return predictions.flatten()\n",
    "\n",
    "print(\"Prediction function defined\")\n",
    "\n",
    "# Test prediction function\n",
    "try:\n",
    "    test_pred = predict_fn(sample_concat[:2])\n",
    "    print(f\"Prediction test successful: shape {test_pred.shape}, values {test_pred}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create SHAP explainer\n",
    "print(\"\\nCreating SHAP KernelExplainer...\")\n",
    "try:\n",
    "    # Use subset for computational efficiency\n",
    "    background_sample = background_concat[:25]\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_sample)\n",
    "    print(f\"Explainer created with {background_sample.shape[0]} background samples\")\n",
    "\n",
    "    # Calculate SHAP values for sample subset\n",
    "    sample_subset = sample_concat[:10]\n",
    "    print(f\"\\nCalculating SHAP values for {sample_subset.shape[0]} patients...\")\n",
    "    print(\"   This may take a few minutes...\")\n",
    "\n",
    "    shap_values = explainer.shap_values(sample_subset, nsamples=100)\n",
    "\n",
    "    # Update sample_concat to match calculated subset\n",
    "    sample_concat = sample_subset\n",
    "\n",
    "    print(f\"SHAP analysis complete!\")\n",
    "    print(f\"   SHAP values shape: {np.array(shap_values).shape}\")\n",
    "    print(f\"   Sample data shape: {sample_concat.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" SHAP calculation failed: {e}\")\n",
    "    print(\"   Consider reducing background_size or sample_size for memory constraints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "f32b357eb3554c6abde075a36db12021",
      "8d408659e9c24d9aaaa2b9e1628b6ff0",
      "84d0c38a8f474a1f864821366c3a6b07",
      "9d3d26200d744d939952261eac201dce",
      "821f0feada264070b75ec97abcdb8c34",
      "f63ac70735bc4f93a33577b1b70c41b5",
      "9ba86f90614548a7980e2f7842f42bb4",
      "7f6800f470c34e0a8aa236e53e1b31ae",
      "f7cb5e2e66ba46d9999a5b2c11feeba8",
      "a3abf35dfa4c49a58f08f5819005ff07",
      "1561410d71b74614a6bc3ebedbc8599e"
     ]
    },
    "id": "Sf-pZNRfeF_E",
    "outputId": "ddd4398e-116d-41e1-eb57-c3e4f73ef9b3"
   },
   "outputs": [],
   "source": [
    "# SHAP Analysis - Calculate Feature Importance\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "print(\"Starting SHAP analysis...\")\n",
    "\n",
    "def predict_fn(data):\n",
    "    \"\"\"\n",
    "    Prediction wrapper for multi-input CNN-LSTM model.\n",
    "    Splits concatenated features back into time series and static components.\n",
    "    \"\"\"\n",
    "    n_ts_features = len(time_series_features)\n",
    "\n",
    "    # Split concatenated data\n",
    "    ts_data = data[:, :n_ts_features]\n",
    "    static_data = data[:, n_ts_features:]\n",
    "\n",
    "    # Expand time series to original temporal dimension\n",
    "    timesteps = X_test_final.shape[1]\n",
    "    ts_data_expanded = np.repeat(ts_data[:, np.newaxis, :], timesteps, axis=1)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict([ts_data_expanded, static_data], verbose=0)\n",
    "    return predictions.flatten()\n",
    "\n",
    "print(\"Prediction function defined\")\n",
    "\n",
    "# Test prediction function\n",
    "try:\n",
    "    test_pred = predict_fn(sample_concat[:2])\n",
    "    print(f\"Prediction test successful: shape {test_pred.shape}, values {test_pred}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create SHAP explainer\n",
    "print(\"\\nCreating SHAP KernelExplainer...\")\n",
    "try:\n",
    "    # Use subset for computational efficiency\n",
    "    background_sample = background_concat[:25]\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_sample)\n",
    "    print(f\"Explainer created with {background_sample.shape[0]} background samples\")\n",
    "\n",
    "    # Calculate SHAP values for sample subset\n",
    "    sample_subset = sample_concat[:10]\n",
    "    print(f\"\\nCalculating SHAP values for {sample_subset.shape[0]} patients...\")\n",
    "    print(\"   This may take a few minutes...\")\n",
    "\n",
    "    shap_values = explainer.shap_values(sample_subset, nsamples=100)\n",
    "\n",
    "    # Update sample_concat to match calculated subset\n",
    "    sample_concat = sample_subset\n",
    "\n",
    "    print(f\"SHAP analysis complete!\")\n",
    "    print(f\"   SHAP values shape: {np.array(shap_values).shape}\")\n",
    "    print(f\"   Sample data shape: {sample_concat.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" SHAP calculation failed: {e}\")\n",
    "    print(\"   Consider reducing background_size or sample_size for memory constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "idvAGUkLhg5p"
   },
   "source": [
    "# SHAP Summary Plot Results\n",
    "\n",
    "*   **Potassium (K) and Heart Rate (HR)** appear to be the most important features, having the largest impact on the model's output.\n",
    "*   For **Potassium**, higher values (red points) tend to be associated with positive SHAP values, pushing the prediction towards a higher risk of mortality.\n",
    "*   For **Heart Rate**, both high and low values seem to have an impact, with high values (red points) generally contributing to higher risk predictions.\n",
    "*   Other important features include **GCS (Glasgow Coma Scale)**, **BUN (Blood Urea Nitrogen)**, **Mechanical Ventilation (MechVent)**, and **SAPS-I**. The impact of these features varies depending on their values. For example, lower GCS scores (blue points) are associated with higher risk.\n",
    "*   Features further down the plot have less overall impact on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Oq549XbheGaa",
    "outputId": "1a581a53-8650-49d7-9ddd-0fb861257a9a"
   },
   "outputs": [],
   "source": [
    "if 'shap_values' in locals() and 'sample_concat' in locals():\n",
    "    print(\"Generating SHAP summary plot...\")\n",
    "\n",
    "    # Handle 3D SHAP values if necessary\n",
    "    if len(np.shape(shap_values)) == 3:\n",
    "        if np.shape(shap_values)[2] > 1:\n",
    "            shap_values_2d = shap_values[:, :, 1]  # Positive class\n",
    "            print(f\"Using SHAP values for positive class. Shape: {np.shape(shap_values_2d)}\")\n",
    "        else:\n",
    "            shap_values_2d = shap_values[:, :, 0]  # Single output\n",
    "            print(f\"Using SHAP values for single output. Shape: {np.shape(shap_values_2d)}\")\n",
    "    else:\n",
    "        shap_values_2d = shap_values\n",
    "        print(f\"Using 2D SHAP values. Shape: {np.shape(shap_values_2d)}\")\n",
    "\n",
    "    # Ensure feature names are available\n",
    "    if 'feature_names' not in locals():\n",
    "        print(\"Warning: Using generic feature names.\")\n",
    "        feature_names = [f'feature_{i}' for i in range(sample_concat.shape[1])]\n",
    "\n",
    "    # Validate dimensions\n",
    "    print(f\"Feature names: {len(feature_names)}, SHAP features: {np.shape(shap_values_2d)[1]}\")\n",
    "\n",
    "    # Generate summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_2d, sample_concat, feature_names=feature_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: SHAP values not found. Run SHAP calculation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "EL-08ihMhxQK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "6fafb3f6"
   },
   "source": [
    "## Final Conclusion\n",
    "\n",
    "Based on the evaluation of the CNN-LSTM model for 36-hour temporal ICU mortality prediction:\n",
    "\n",
    "**Results Summary:**\n",
    "\n",
    "*   **Overall Performance:** The model demonstrates moderate discriminatory ability with an AUROC of 0.77. The AUPRC of 0.36 suggests limitations in achieving high precision and recall simultaneously, particularly in correctly identifying positive cases without a high number of false positives.\n",
    "*   **Confusion Matrix:** The confusion matrix highlights that while the model correctly identifies a large number of survivors (True Negatives = 390), it also has a significant number of false positives (299), meaning it predicts death for many patients who survive. The number of false negatives (19) is relatively low, indicating the model is less likely to miss a patient who dies.\n",
    "*   **Feature Importance (SHAP):** The SHAP analysis reveals that **Potassium (K)** and **Heart Rate (HR)** are the most influential features in the model's predictions. High Potassium and high Heart Rate values are associated with an increased risk of mortality. Other important features include **GCS**, **BUN**, **Mechanical Ventilation**, and **SAPS-I**, with lower GCS scores contributing to higher risk.\n",
    "*   **Early Prediction Capability:** The current model demonstrates the ability to achieve good accuracy and recall using only the first 36 hours of ICU data. This early prediction capability is crucial for enabling faster clinical interventions and potentially improving patient outcomes by allowing healthcare providers to identify high-risk patients sooner.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "*   **Resource Allocation:** The model could help identify patients at high risk of mortality, allowing hospitals to prioritize resources, such as staffing, equipment, and interventions, to improve patient outcomes and potentially reduce costs associated with prolonged ICU stays.\n",
    "*   **Early Intervention:** By predicting mortality risk early (within 36 hours), the model could alert clinicians to patients who may benefit from more aggressive or timely interventions, potentially preventing deterioration and improving survival rates.\n",
    "*   **Clinical Decision Support:** The model's predictions and feature importance insights could serve as a valuable tool for clinicians, providing data-driven support for complex decisions related to patient care, prognostication, and family discussions.\n",
    "*   **Quality Improvement:** Analyzing model performance and common false positives/negatives could help identify areas for improvement in clinical protocols, data collection, and patient management within the ICU.\n",
    "*   **Benchmarking and Performance Monitoring:** The model could be used to benchmark the performance of different ICUs or track changes in mortality rates over time, providing insights for quality improvement initiatives."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
