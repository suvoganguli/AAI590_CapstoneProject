{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0059aea4-2bc9-404f-92cc-638914fd9b90",
   "metadata": {},
   "source": [
    "# 04a: Data Preparation for XGBoost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b770a1b-ac18-482e-8fee-af4c1f92e431",
   "metadata": {},
   "source": [
    "## Goal for this notebook\n",
    "\n",
    "The goal of this notebook is to take the cleaned, aggregated dataset and prepare it for training our XGBoost model. This involves separating the features from the target variable, encoding categorical features appropriately, splitting the data into training and testing sets, and addressing the significant class imbalance using an over-sampling technique (SMOTE). The final output will be data splits that are ready for the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9950d1a-0ca8-4cfe-b61e-50064cad8109",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We'll start by importing the necessary libraries from pandas and scikit-learn, and then load the `aggregated_cleaned.csv` file created in the previous data cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28439a2-7f28-47c4-aa91-1af34027ae69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.3.2\n",
      "Uninstalling scikit-learn-1.3.2:\n",
      "  Successfully uninstalled scikit-learn-1.3.2\n",
      "Found existing installation: imbalanced-learn 0.12.4\n",
      "Uninstalling imbalanced-learn-0.12.4:\n",
      "  Successfully uninstalled imbalanced-learn-0.12.4\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.2.0)\n",
      "Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (2.2.0)\n",
      "Using cached imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.4\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you get import-error later, and then restart kernel\n",
    "!pip uninstall -y scikit-learn imbalanced-learn\n",
    "!pip install scikit-learn\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9c0a5f-8330-4828-b1ae-83a7498f2d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'aggregated_cleaned.csv'.\n",
      "Dataset shape: (4000, 175)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "# Load the cleaned data from the previous step\n",
    "df = pd.read_csv('../data/processed/aggregated_cleaned.csv')\n",
    "print(\"Successfully loaded 'aggregated_cleaned.csv'.\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f55d3f-f80c-49c6-83c2-7e4254dcd1ba",
   "metadata": {},
   "source": [
    "## 2. Feature and Target Separation\n",
    "\n",
    "First, we need to separate our dataset into the feature matrix (X), which contains all the predictor variables, and the target vector (y), which is the In-hospital_death column we want to predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e4539f-dbdc-4d6c-b2e9-eda83cd68171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix (X): (4000, 173)\n",
      "Shape of target vector (y): (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Define the target variable\n",
    "target = 'In-hospital_death'\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[target, 'RecordID']) # Drop target and identifier\n",
    "y = df[target]\n",
    "\n",
    "print(f\"Shape of feature matrix (X): {X.shape}\")\n",
    "print(f\"Shape of target vector (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d5f14-6011-4d7f-8c10-9581c10b994b",
   "metadata": {},
   "source": [
    "## 3. Categorical Feature Encoding\n",
    "\n",
    "Our EDA identified ICUType and Gender as categorical features. To prevent the model from assuming a false numerical order, we will convert them into a format it can understand using one-hot encoding. This creates a new binary column for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cad017-751f-4839-a455-428a2cc01ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X: (4000, 173)\n",
      "Shape of X after one-hot encoding: (4000, 176)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>SAPS-I</th>\n",
       "      <th>SOFA</th>\n",
       "      <th>Weight</th>\n",
       "      <th>ALP_mean</th>\n",
       "      <th>ALP_min</th>\n",
       "      <th>ALP_max</th>\n",
       "      <th>ALP_count</th>\n",
       "      <th>ALT_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>pH_mean</th>\n",
       "      <th>pH_std</th>\n",
       "      <th>pH_min</th>\n",
       "      <th>pH_max</th>\n",
       "      <th>pH_count</th>\n",
       "      <th>ICUType_2.0</th>\n",
       "      <th>ICUType_3.0</th>\n",
       "      <th>ICUType_4.0</th>\n",
       "      <th>Gender_0.0</th>\n",
       "      <th>Gender_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.0</td>\n",
       "      <td>185.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>229.00</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.394333</td>\n",
       "      <td>0.040302</td>\n",
       "      <td>7.374</td>\n",
       "      <td>7.408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.55</td>\n",
       "      <td>125.6</td>\n",
       "      <td>131.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.420983</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>7.396</td>\n",
       "      <td>7.442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>55.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.418000</td>\n",
       "      <td>0.039983</td>\n",
       "      <td>7.386</td>\n",
       "      <td>7.462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>154.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>70.60</td>\n",
       "      <td>67.0</td>\n",
       "      <td>74.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.374706</td>\n",
       "      <td>0.052929</td>\n",
       "      <td>7.270</td>\n",
       "      <td>7.440</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>74.3</td>\n",
       "      <td>258.00</td>\n",
       "      <td>216.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>240.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>7.430000</td>\n",
       "      <td>0.039158</td>\n",
       "      <td>7.390</td>\n",
       "      <td>7.480</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Height  SAPS-I  SOFA  Weight  ALP_mean  ALP_min  ALP_max  ALP_count  \\\n",
       "0  52.0   185.4     9.0   2.0    90.0    229.00    229.0    229.0        1.0   \n",
       "1  65.0    -1.0    11.0   3.0    -1.0    128.55    125.6    131.4        0.0   \n",
       "2  47.0    -1.0     4.0   1.0    86.6     55.00     55.0     55.0        1.0   \n",
       "3  35.0   154.9    -1.0   7.0    67.0     70.60     67.0     74.2        0.0   \n",
       "4  64.0    -1.0    -1.0  -1.0    74.3    258.00    216.0    300.0        2.0   \n",
       "\n",
       "     ALT_mean  ...   pH_mean    pH_std  pH_min  pH_max  pH_count  ICUType_2.0  \\\n",
       "0    5.000000  ...  7.394333  0.040302   7.374   7.408       0.0            0   \n",
       "1   40.250000  ...  7.420983  0.021805   7.396   7.442       0.0            1   \n",
       "2   68.000000  ...  7.418000  0.039983   7.386   7.462       0.0            0   \n",
       "3   29.200000  ...  7.374706  0.052929   7.270   7.440      17.0            1   \n",
       "4  240.333333  ...  7.430000  0.039158   7.390   7.480       4.0            0   \n",
       "\n",
       "   ICUType_3.0  ICUType_4.0  Gender_0.0  Gender_1.0  \n",
       "0            0            0           0           1  \n",
       "1            0            0           0           1  \n",
       "2            0            0           1           0  \n",
       "3            0            0           1           0  \n",
       "4            1            0           1           0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['ICUType', 'Gender']\n",
    "\n",
    "print(f\"Original shape of X: {X.shape}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"Shape of X after one-hot encoding: {X_encoded.shape}\")\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941aae53-8ed3-4ee5-ab6c-2db6ccbf138b",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "We will now split our data into training and testing sets. The model will be trained on the training set, and its performance will be evaluated on the unseen test set. We use stratification to ensure the proportion of the minority class is the same in both splits, which is crucial for our imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47901741-8d2c-4f98-9de7-a79937359552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "X_train shape: (3200, 176)\n",
      "X_test shape: (800, 176)\n",
      "Original y_train distribution:\n",
      "0    0.861563\n",
      "1    0.138437\n",
      "Name: In-hospital_death, dtype: float64\n",
      "\n",
      "y_test distribution:\n",
      "0    0.86125\n",
      "1    0.13875\n",
      "Name: In-hospital_death, dtype: float64\n",
      "\n",
      "Applying SMOTE to the training data...\n",
      "\n",
      "Resampled (SMOTE) training set shape: (5514, 176)\n",
      "Resampled training set distribution:\n",
      "0    2757\n",
      "1    2757\n",
      "Name: In-hospital_death, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using the encoded X and original y before any oversampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y # Stratify is crucial for imbalanced datasets\n",
    ")\n",
    "\n",
    "print(\"Data split into training and testing sets.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Original y_train distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\ny_test distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nResampled (SMOTE) training set shape: {X_train_smote.shape}\")\n",
    "print(f\"Resampled training set distribution:\\n{y_train_smote.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70603c-c187-41ff-81de-ccae6ece5842",
   "metadata": {},
   "source": [
    "## 5. Handle Class Imbalance with SMOTE\n",
    "\n",
    "To address the significant class imbalance, we will apply the Synthetic Minority Over-sampling Technique (SMOTE). This technique generates new, synthetic data points for the minority class (mortality) to create a balanced dataset for the model to learn from. Crucially, this is only applied to the training data to prevent data leakage and ensure our test set remains representative of the real-world data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81ee3ab-4dee-45c8-b647-78dd8837293c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set shape: (3200, 176)\n",
      "Original training set distribution:\n",
      "0    2757\n",
      "1     443\n",
      "Name: In-hospital_death, dtype: int64\n",
      "\n",
      "Applying SMOTE to the training set...\n",
      "Resampled training set shape: (5514, 176)\n",
      "Resampled training set distribution:\n",
      "0    2757\n",
      "1    2757\n",
      "Name: In-hospital_death, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training set shape: {X_train.shape}\")\n",
    "print(f\"Original training set distribution:\\n{y_train.value_counts()}\")\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training set...\")\n",
    "print(f\"Resampled training set shape: {X_train_smote.shape}\")\n",
    "print(f\"Resampled training set distribution:\\n{y_train_smote.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f2b05-1d0b-4b44-ad34-15b4dfaaef3c",
   "metadata": {},
   "source": [
    "## 6. Save Prepared Data\n",
    "\n",
    "Finally, we save our prepared data splits. We will save both the original splits and the SMOTE-resampled training split, allowing us to compare modeling approaches in the next notebook. Using `pickle` is a good way to preserve the DataFrame structure and data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a7afe1-be45-4d01-b5ed-142fc956b1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared data splits saved to 'aggregated_data_splits.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Define the output path\n",
    "output_dir = '../data/features/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the data splits using pickle\n",
    "with open(os.path.join(output_dir, 'aggregated_data_splits.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'X_train_smote': X_train_smote,\n",
    "        'y_train_smote': y_train_smote\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nPrepared data splits saved to 'aggregated_data_splits.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff67b55-08c7-4a79-b3df-cc608aaf100e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
