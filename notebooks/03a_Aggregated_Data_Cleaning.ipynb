{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37657671-cec3-4fab-ad25-e83085f50d28",
   "metadata": {},
   "source": [
    "# 03a: Data Cleaning (Aggregated Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389ede2-bb5a-4778-b3c1-49b5cdbca5e9",
   "metadata": {},
   "source": [
    "## Goal for this notebook\n",
    "\n",
    "The goal of this notebook is to perform the necessary data cleaning on the aggregated feature set. This involves loading the data, handling data leakage by removing outcome-related variables, addressing columns with a high percentage of missing data, and imputing the remaining missing values. The final output will be a clean, complete dataset ready for the model preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d66f95-33ea-41e9-a433-0fc04867dcb8",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "We'll start by importing the necessary libraries and loading the aggregated feature dataset created in the ingestion phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338dfd51-f415-4aef-8f77-728b038766de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'patient_aggregated_features_df.csv'.\n",
      "Original dataset shape: (4000, 191)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import os\n",
    "\n",
    "# Load the processed data from the previous step\n",
    "df = pd.read_csv('../data/processed/patient_aggregated_features_df.csv')\n",
    "print(\"Successfully loaded 'patient_aggregated_features_df.csv'.\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3a10e-23f7-4211-b303-77e6c370cb69",
   "metadata": {},
   "source": [
    "## 2. Handle Data Leakage\n",
    "\n",
    "**Finding**: Our dataset contains features like Survival and Length_of_stay which are determined at the end of a patient's stay. To build a realistic predictive model that can make predictions early in an ICU admission, this information (which comes from the \"future\") must be removed to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f0448a-836c-4b03-8aa1-91efe2e8e1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed leaky features: ['Survival', 'Length_of_stay']\n",
      "Shape after removing leaky features: (4000, 189)\n"
     ]
    }
   ],
   "source": [
    "leaky_features = ['Survival', 'Length_of_stay']\n",
    "\n",
    "# Check which leaky features are in the dataframe before dropping\n",
    "features_to_drop = [feat for feat in leaky_features if feat in df.columns]\n",
    "\n",
    "if features_to_drop:\n",
    "    df.drop(columns=features_to_drop, inplace=True)\n",
    "    print(f\"Removed leaky features: {features_to_drop}\")\n",
    "    print(f\"Shape after removing leaky features: {df.shape}\")\n",
    "else:\n",
    "    print(\"No leaky features found to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cba0c-c310-4205-b248-6550cecf493f",
   "metadata": {},
   "source": [
    "## 3. Handle High-Missingness Features\n",
    "\n",
    "**Finding**: The EDA revealed that several features, mostly related to specialized lab tests, are missing for over 80% of patients. Imputing this much data would be unreliable. Therefore, we will remove these columns to avoid introducing noise into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cc3a1d-2b7b-4d43-a205-3f463506c90c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 14 columns with >80% missing values.\n",
      "Shape after removing high-missingness columns: (4000, 175)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing values\n",
    "missing_percent = df.isnull().mean()\n",
    "\n",
    "# Identify columns to drop (missing > 80%)\n",
    "cols_to_drop_missing = missing_percent[missing_percent > 0.8].index\n",
    "\n",
    "if not cols_to_drop_missing.empty:\n",
    "    df.drop(columns=cols_to_drop_missing, inplace=True)\n",
    "    print(f\"Removed {len(cols_to_drop_missing)} columns with >80% missing values.\")\n",
    "    print(f\"Shape after removing high-missingness columns: {df.shape}\")\n",
    "else:\n",
    "    print(\"No columns with >80% missing values to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84504efb-a2e9-40f2-b6e4-f9179ad98f66",
   "metadata": {},
   "source": [
    "## 4. Impute Remaining Missing Values\n",
    "\n",
    "For the remaining features with missing data, we will use K-Nearest Neighbors (KNN) imputation. This method estimates a missing value based on the values of the 'k' most similar patients, which can lead to more realistic imputations than using a simple mean or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244cc4bd-44c3-4f61-9e37-b6b851c754a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing remaining missing values with KNNImputer...\n",
      "Imputation complete.\n",
      "Final cleaned shape: (4000, 175)\n",
      "Missing values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "# Separate identifiers and the target variable, which should not be imputed\n",
    "ids_and_target = df[['RecordID', 'In-hospital_death']]\n",
    "features_to_impute = df.drop(columns=['RecordID', 'In-hospital_death'])\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit and transform the feature data\n",
    "print(\"Imputing remaining missing values with KNNImputer...\")\n",
    "imputed_features = imputer.fit_transform(features_to_impute)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "df_imputed_features = pd.DataFrame(imputed_features, columns=features_to_impute.columns)\n",
    "\n",
    "# Combine the imputed features with the identifiers and target\n",
    "df_cleaned = pd.concat([ids_and_target.reset_index(drop=True), df_imputed_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Imputation complete.\")\n",
    "print(f\"Final cleaned shape: {df_cleaned.shape}\")\n",
    "print(f\"Missing values remaining: {df_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254c093-b97c-4802-ae31-c9397e7a3666",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "Finally, we save the fully cleaned and imputed DataFrame to a new CSV file. This file will be the input for the next stage of our pipeline: model preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6ad301-33d4-4bb9-ab20-4f9a5ab37ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data saved to: ../data/processed/aggregated_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output path\n",
    "output_dir = '../data/processed/'\n",
    "output_file = os.path.join(output_dir, 'aggregated_cleaned.csv')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataframe\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nCleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58e4b0-9f06-4869-adcd-7cf1536d05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
